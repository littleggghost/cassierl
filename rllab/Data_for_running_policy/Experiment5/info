This was trained from scratch with the modified rewards.
This is similar to Experiment 4. However, the rewards have been weighted differently.

Experiment6 is resumed from this training, and doing it for 250 more epochs.

# Network
hidden_sizes=(32, 32),
init_std=5.0

batch_size=10000,
max_path_length=5000, # dt = 1/2000
n_itr=250,
discount=0.99,
step_size=0.01

#reward
r = -5*(0.9 - self.xstate.body_x[1])**2      # penalty for body height error squared
r -= 0.05*((sp[5] + sp[11])/2.0) ** 2        # penalty to feet not being over COM
r -= 0.001*np.sum(action ** 2)               # to reduce jerkiness, cost on accelerations
r -= 0.05*(0.0 - self.xstate.body_x[2])**2   # penalty on body pitch